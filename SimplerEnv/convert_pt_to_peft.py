#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ checkpoint .pt —Ñ–∞–π–ª–∞ –∏–∑ rl-research –≤ —Ñ–æ—Ä–º–∞—Ç PEFT.

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python convert_pt_to_peft.py \
        --checkpoint /path/to/rl-research/runs/checkpoints/last23.pt \
        --vla_path models/openvla-7b-fixed \
        --output_dir ./converted_lora \
        [--lora_rank 16] \
        [--lora_alpha 32] \
        [--unnorm_key bridge_orig]
"""

import argparse
import json
from pathlib import Path

import torch
from peft import LoraConfig, PeftModel, get_peft_model
from prismatic.extern.hf.modeling_prismatic import \
    OpenVLAForActionPredictionWithValueHead
from prismatic.extern.hf.processing_prismatic import (PrismaticImageProcessor,
                                                      PrismaticProcessor)
from transformers import AutoTokenizer


def convert_pt_to_peft(
    checkpoint_path: str,
    vla_path: str,
    output_dir: str,
    lora_rank: int = None,
    lora_alpha: int = None,
    lora_dropout: float = 0.0,
    unnorm_key: str = "bridge_orig",
    device: str = "cuda:0",
):
    """
    –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç checkpoint .pt —Ñ–∞–π–ª –∏–∑ rl-research –≤ —Ñ–æ—Ä–º–∞—Ç PEFT.
    
    Args:
        checkpoint_path: –ü—É—Ç—å –∫ .pt —Ñ–∞–π–ª—É checkpoint
        vla_path: –ü—É—Ç—å –∫ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ OpenVLA
        output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è PEFT –º–æ–¥–µ–ª–∏
        lora_rank: Rank LoRA (–µ—Å–ª–∏ None, –±—É–¥–µ—Ç –∏–∑–≤–ª–µ—á–µ–Ω –∏–∑ checkpoint)
        lora_alpha: Alpha LoRA (–µ—Å–ª–∏ None, –±—É–¥–µ—Ç –∏–∑–≤–ª–µ—á–µ–Ω –∏–∑ checkpoint)
        lora_dropout: Dropout LoRA
        unnorm_key: –ö–ª—é—á –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–µ–π—Å—Ç–≤–∏–π
        device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
    """
    checkpoint_path = Path(checkpoint_path)
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ checkpoint: {checkpoint_path}")
    checkpoint = torch.load(checkpoint_path, map_location="cpu", weights_only=False)
    
    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ LoRA –≤–µ—Å–æ–≤
    if "actor_lora_state_dict" not in checkpoint:
        raise ValueError("Checkpoint –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç 'actor_lora_state_dict'!")
    
    lora_state_dict = checkpoint["actor_lora_state_dict"]
    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(lora_state_dict)} LoRA –≤–µ—Å–æ–≤")
    
    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ LoRA –∏–∑ checkpoint –∏–ª–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    config = checkpoint.get("config", {})
    if lora_rank is None:
        lora_rank = config.get("lora_rank", 32)
    if lora_alpha is None:
        lora_alpha = config.get("lora_alpha", min(lora_rank, 16))
    
    print(f"üìã LoRA –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: rank={lora_rank}, alpha={lora_alpha}, dropout={lora_dropout}")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
    print(f"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏: {vla_path}")
    device_obj = torch.device(device)
    
    image_processor = PrismaticImageProcessor.from_pretrained(vla_path, trust_remote_code=True)
    tokenizer = AutoTokenizer.from_pretrained(vla_path, trust_remote_code=True, padding_side="left")
    processor = PrismaticProcessor.from_pretrained(
        vla_path,
        image_processor=image_processor,
        tokenizer=tokenizer,
        trust_remote_code=True
    )
    
    vla = OpenVLAForActionPredictionWithValueHead.from_pretrained(
        vla_path,
        attn_implementation="flash_attention_2",
        torch_dtype=torch.bfloat16,
        low_cpu_mem_usage=True,
        trust_remote_code=True,
        device_map=device,
        vh_mode="a0",
    )
    
    # –°–æ–∑–¥–∞–Ω–∏–µ PEFT –º–æ–¥–µ–ª–∏ —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
    print("üîß –°–æ–∑–¥–∞–Ω–∏–µ PEFT –º–æ–¥–µ–ª–∏...")
    lora_config = LoraConfig(
        r=lora_rank,
        lora_alpha=lora_alpha,
        lora_dropout=lora_dropout,
        target_modules=[
            "proj", "qkv", "fc1", "fc2",  # vision
            "q", "kv", "fc3",  # project
            "q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj", "lm_head",  # llm
        ],
        init_lora_weights="gaussian"
    )
    
    peft_model = get_peft_model(vla, lora_config)
    peft_model.print_trainable_parameters()
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ LoRA –≤–µ—Å–æ–≤
    print("üì• –ó–∞–≥—Ä—É–∑–∫–∞ LoRA –≤–µ—Å–æ–≤ –∏–∑ checkpoint...")
    # –ö–ª—é—á–∏ –≤ checkpoint —É–∂–µ –∏–º–µ—é—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç: base_model.model.language_model.model.layers...
    # PEFT –º–æ–¥–µ–ª—å –æ–∂–∏–¥–∞–µ—Ç —Ç–∞–∫–æ–π –∂–µ —Ñ–æ—Ä–º–∞—Ç, —Ç–∞–∫ —á—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–ª—é—á–∏ –∫–∞–∫ –µ—Å—Ç—å
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞ –Ω–∞–ø—Ä—è–º—É—é
    missing_keys, unexpected_keys = peft_model.load_state_dict(lora_state_dict, strict=False)
    
    if missing_keys:
        print(f"‚ö†Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∫–ª—é—á–∏ ({len(missing_keys)}): {missing_keys[:5]}...")
    if unexpected_keys:
        print(f"‚ö†Ô∏è  –ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–µ –∫–ª—é—á–∏ ({len(unexpected_keys)}): {unexpected_keys[:5]}...")
    
    if not missing_keys and not unexpected_keys:
        print("‚úÖ –í—Å–µ LoRA –≤–µ—Å–∞ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã!")
    else:
        print("‚ö†Ô∏è  –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –∫–ª—é—á–∏ –Ω–µ —Å–æ–≤–ø–∞–ª–∏, –Ω–æ —ç—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–æ—Ä–º–∞–ª—å–Ω–æ")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ PEFT
    print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ PEFT –º–æ–¥–µ–ª–∏ –≤: {output_dir}")
    peft_model.save_pretrained(str(output_dir))
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ dataset_statistics.json
    dataset_stats_saved = False
    if "norm_stats" in config:
        norm_stats = config["norm_stats"]
        if unnorm_key in norm_stats:
            dataset_stats = {unnorm_key: norm_stats[unnorm_key]}
            with open(output_dir / "dataset_statistics.json", "w") as f:
                json.dump(dataset_stats, f, indent=2)
            print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω dataset_statistics.json —Å –∫–ª—é—á–æ–º '{unnorm_key}'")
            dataset_stats_saved = True
        else:
            available_keys = list(norm_stats.keys())
            print(f"‚ö†Ô∏è  –ö–ª—é—á '{unnorm_key}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ norm_stats")
            print(f"   –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–ª—é—á–∏: {available_keys}")
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –∫–ª—é—á–∏
            if norm_stats:
                with open(output_dir / "dataset_statistics.json", "w") as f:
                    json.dump(norm_stats, f, indent=2)
                print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω dataset_statistics.json —Å–æ –≤—Å–µ–º–∏ –¥–æ—Å—Ç—É–ø–Ω—ã–º–∏ –∫–ª—é—á–∞–º–∏")
                print(f"   üí° –ï—Å–ª–∏ –Ω—É–∂–µ–Ω –¥—Ä—É–≥–æ–π –∫–ª—é—á, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ --unnorm_key —Å –æ–¥–Ω–∏–º –∏–∑: {available_keys}")
                dataset_stats_saved = True
    else:
        print("‚ö†Ô∏è  norm_stats –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ checkpoint")
    
    if not dataset_stats_saved:
        print("‚ö†Ô∏è  dataset_statistics.json –Ω–µ –±—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω")
    
    print(f"\n‚úÖ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {output_dir}")
    print(f"\nüí° –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ run_eval.sh:")
    print(f"   vla_load_path=\"{output_dir}\"")


def main():
    parser = argparse.ArgumentParser(
        description="–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è checkpoint .pt –∏–∑ rl-research –≤ —Ñ–æ—Ä–º–∞—Ç PEFT"
    )
    parser.add_argument(
        "--checkpoint",
        type=str,
        required=True,
        help="–ü—É—Ç—å –∫ .pt —Ñ–∞–π–ª—É checkpoint –∏–∑ rl-research"
    )
    parser.add_argument(
        "--vla_path",
        type=str,
        required=True,
        help="–ü—É—Ç—å –∫ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ OpenVLA"
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        required=True,
        help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è PEFT –º–æ–¥–µ–ª–∏"
    )
    parser.add_argument(
        "--lora_rank",
        type=int,
        default=None,
        help="LoRA rank (–µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω, –±—É–¥–µ—Ç –∏–∑–≤–ª–µ—á–µ–Ω –∏–∑ checkpoint)"
    )
    parser.add_argument(
        "--lora_alpha",
        type=int,
        default=None,
        help="LoRA alpha (–µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω, –±—É–¥–µ—Ç –∏–∑–≤–ª–µ—á–µ–Ω –∏–∑ checkpoint)"
    )
    parser.add_argument(
        "--lora_dropout",
        type=float,
        default=0.0,
        help="LoRA dropout"
    )
    parser.add_argument(
        "--unnorm_key",
        type=str,
        default="bridge_orig",
        help="–ö–ª—é—á –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–µ–π—Å—Ç–≤–∏–π"
    )
    parser.add_argument(
        "--device",
        type=str,
        default="cuda:0",
        help="–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ (cuda:0, cpu, etc.)"
    )
    
    args = parser.parse_args()
    
    convert_pt_to_peft(
        checkpoint_path=args.checkpoint,
        vla_path=args.vla_path,
        output_dir=args.output_dir,
        lora_rank=args.lora_rank,
        lora_alpha=args.lora_alpha,
        lora_dropout=args.lora_dropout,
        unnorm_key=args.unnorm_key,
        device=args.device,
    )


if __name__ == "__main__":
    main()

